
# I/O虚拟化

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/fc075282ceed4070ab839270a8d01c42.png)



**全虚拟化**：宿主机截获客户机对I/O设备的访问请求，然后通过软件模拟真实的硬件。这种方式对客户机而言非常透明，无需考虑底层硬件的情况，不需要修改操作系统。

**半虚拟化**：通过前端驱动/后端驱动模拟实现I/O虚拟化。客户机中的驱动程序为前端，宿主机提供的与客户机通信的驱动程序为后端。前端驱动将客户机的请求通过与宿主机间的特殊通信机制发送给后端驱动，后端驱动在处理完请求后再发送给物理驱动。
  
**IO透传**：直接把物理设备分配给虚拟机使用，这种方式需要硬件平台具备I/O透传技术，例如Intel VT-d技术。它能获得近乎本地的性能，并且CPU开销不高。

DPDK支持半虚拟化的前端virtio和后端vhost，并且对前后端都有性能加速的设计。而对于I/O透传，DPDK可以直接在客户机里使用，就像在宿主机里，直接接管物理设备，进行操作。


## I/O透传

I/O透传技术（如SR-IOV）通过直接将硬件设备的访问权限分配给虚拟机（VM），提供高性能的网络和存储操作。这主要是利用Intel® VT-d的支持，减少虚拟机操作时对宿主机的干预，从而避免性能瓶颈。

>Intel® VT-d：该技术允许虚拟机直接访问物理设备，减少了传统虚拟化中常见的“VM-Exit”（虚拟机退出到宿主机的开销）
>SR-IOV（单根I/O虚拟化）：允许一个物理网卡通过虚拟化生成多个虚拟网卡。这些虚拟网卡可以分配给不同的虚拟机，使每个虚拟机能获得几乎独立的网络设备访问，从而提升性能。

VT-d主要给宿主机软件提供了以下的功能：

- I/O设备的分配：可以灵活地把I/O设备分配给虚拟机，把对虚拟机的保护和隔离的特性扩展到I/O的操作上来。

- DMA重映射：可以支持来自设备DMA的地址翻译转换。

- 中断重映射：可以支持来自设备或者外部中断控制器的中断的隔离和路由到对应的虚拟机。
- 可靠性：记录并报告DMA和中断的错误给系统软件，否则的话可能会破坏内存或影响虚拟机的隔离。


## SR-IOV

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8c5203420e7a46b28a210fb612ddcb42.png)


SR-IOV技术是由PCI-SIG制定的一套硬件虚拟化规范，全称是Single Root IO Virtualization（单根IO虚拟化）。SR-IOV规范主要用于网卡（NIC）、磁盘阵列控制器（RAID controller）和光纤通道主机总线适配器（Fibre Channel Host Bus Adapter，FC HBA），使数据中心达到更高的效率。SR-IOV架构中，一个I/O设备支持最多256个虚拟功能，同时将每个功能的硬件成本降至最低。SR-IOV引入了两个功能类型：


- PF（Physical Function，物理功能）：这是支持SR-IOV扩展功能的PCIe功能，主要用于配置和管理SR-IOV，拥有所有的PCIe设备资源。PF在系统中不能被动态地创建和销毁（PCI Hotplug除外）。
- VF（Virtual Function，虚拟功能）：“精简”的PCIe功能，包括数据迁移必需的资源，以及经过谨慎精简的配置资源集，可以通过PF创建和销毁。


![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/492141ab46f342609f397ee481036419.png)
# Virtio网络设备

在客户机操作系统中实现的前端驱动程序一般直接叫Virtio，在宿主机实现的后端驱动程序目前常用的叫Vhost。
## Linux内核驱动设计

Virtio网络设备Linux内核驱动主要包括三个层次：底层PCI-e设备层，中间Virtio虚拟队列层，上层网络设备层。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/556fe40e7db74554823c4aa0b00bff03.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c1e50a98719d4a9e92c8775840234370.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/64d3802e2f5246f1a1b8a15cf362b1a2.png)

## DPDK用户空间virtio设备的优化

DPDK用户空间驱动和Linux内核驱动相比，主要不同点在于DPDK只暂时实现了Virtio网卡设备，所以整个构架和优化上面可以暂时只考虑网卡设备的应用场景。


**关于单帧mbuf的网络包收发优化**：将描述符表和可用环表（用于管理数据包）的映射关系固定。这意味着每个环表项（用于描述网络数据包的内存位置）和描述符表中的位置是一一对应的。在接收数据时，每个可用环表项固定指向一个描述符表的位置；发送数据时，描述符表的映射被优化以加速数据处理。固定映射减少了不同CPU核之间的缓存迁移开销，也节省了描述符表的分配和释放操作。


**Indirect特性在网络包发送中的支持**：发送网络包时，通常需要两个描述符来管理一个数据包：一个描述符用于指向数据包本身，另一个用于管理数据包的头部信息。支持间接描述符的特性允许将这两个描述符的需求简化为一个描述符。这个描述符指向一个额外分配的内存区域（间接描述符表），这个表可以包含多个描述符的信息。通过这种方式，不论是发送单个小数据包还是巨型帧（大数据包），只需要一个描述符就能管理。这减少了描述符的开销和管理复杂性，提高了发送效率。


![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c5098c03ebb84fc48fffdb07d3946ab2.png)
# Vhost
Virtio-net 的后端驱动经历了以下演进过程：

- Virtio-net 后端：最初的虚拟网络设备驱动直接在虚拟化管理程序中实现，用于处理虚拟机中的网络数据。

- 内核态 Vhost-net：为了提高性能，引入了 Vhost-net 框架，将数据路径的一部分移到内核空间，减少了用户态和内核态之间的开销。

- 用户态 Vhost-user：进一步提高性能和灵活性，通过将 Vhost 的实现移到用户态，使虚拟化管理程序可以更灵活地管理网络数据，同时保持高效的性能。

Vhost 是一个框架，它最初在内核态实现了高效的虚拟设备操作，后续的 Vhost-user 将这个框架扩展到了用户态，以支持更灵活的虚拟化场景。

## virtio-net

virtio-net后端驱动的最基本要素是虚拟队列机制、消息通知机制和中断机制。虚拟队列机制连接着客户机和宿主机的数据交互。消息通知机制主要用于从客户机到宿主机的消息通知。中断机制主要用于从宿主机到客户机的中断请求和处理。

>Tap设备（Tap Interface）是一个虚拟网络接口，用于将虚拟机的网络流量传输到宿主机的网络栈。它通常用于虚拟化环境中，使虚拟机能够与宿主机或其他网络设备进行通信。


>QEMU（Quick Emulator）是一个开源的虚拟机管理程序和模拟器。它可以模拟各种硬件平台，并提供虚拟化支持，使得多个虚拟机可以在一个物理机上运行。QEMU通过处理虚拟机的硬件请求、管理虚拟机的资源来实现虚拟化。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/547ee9da2d13464dbdfb2eeb7ac9f837.png)

数据通道的瓶颈：
- 从Tap设备到QEMU：Tap设备是虚拟网络接口，用于处理虚拟机的网络流量。数据包从Tap设备接收后，需要复制到QEMU（虚拟机管理程序）。这意味着数据包需要经过一个拷贝操作才能从Tap设备转移到QEMU。
- 从QEMU到客户机：QEMU处理完数据包后，再将其发送到虚拟机的网络栈。这也需要一个拷贝操作。这两个拷贝操作（Tap设备到QEMU，QEMU到虚拟机）都增加了延迟和处理开销，成为性能瓶颈。

消息通知路径的瓶颈：
- 内核到QEMU的通知消息：当数据包到达Tap设备时，内核需要通知QEMU有新的数据可供处理。这个通知过程涉及内核发出一个消息，告知QEMU有数据到达。
- IOCTL请求和中断：QEMU在接收到内核的通知后，会使用IOCTL（输入输出控制）调用请求KVM（内核虚拟机管理程序）发送一个中断。中断用于通知虚拟机有新的数据到达。
- KVM到客户机的中断：KVM接到QEMU的中断请求后，向虚拟机发送中断信号。这个中断信号让虚拟机知道有新的数据包需要处理。
## Linux内核态vhost-net


vhost-net通过卸载virtio-net在报文收发处理上的工作，使Qemu从virtio-net的虚拟队列工作中解放出来，减少上下文切换和数据包拷贝，进而提高报文收发的性能。除此以外，宿主机上的vhost-net模块还需要承担报文到达和发送消息通知及中断的工作。

报文接收仍然包括数据通路和消息通知路径两个方面：

- 数据通路是从Tap设备接收数据报文，通过vhost-net模块把该报文拷贝到虚拟队列中的数据区，从而使客户机接收报文。
- 消息通路是当报文从Tap设备到达vhost-net时，通过KVM模块向客户机发送中断，通知客户机接收报文。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b32aea85787a4af1941b4d24dc1b4818.png)

## 用户态vhost

Linux内核态的vhost-net模块需要在内核态完成报文拷贝和消息处理，这会给报文处理带来一定的性能损失，因此用户态的vhost应运而生。用户态vhost采用了共享内存技术，通过共享的虚拟队列来完成报文传输和控制，大大降低了vhost和virtio-net之间的数据传输成本。


数据通路不再涉及内核，直接通过共享内存发送给用户态应用（如DPDK，OVS）

消息通路通过Unix Domain Socket实现，是一种在同一台计算机上进程间通信（IPC）的方法。与网络套接字不同，它不涉及网络协议栈，数据通过内存中的文件描述符进行传输。

## DPDK vhost

DPDK vhost支持vhost-cuse（用户态字符设备）和vhost-user（用户态socket服务）两种消息机制，它负责为客户机中的virtio-net创建、管理和销毁vhost设备。

当使用vhost-user时，首先需要在系统中创建一个Unix domain socket server，用于处理Qemu发送给vhost的消息。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e884b134c72b4e9da053a734f979f819.png)

DPDK示例程序vhost-switch是基于vhost lib的一个用户态以太网交换机的实现，可以完成在virtio-net设备和物理网卡之间的报文交换。还使用了虚拟设备队列（VMDQ）技术来减少交换过程中的软件开销，该技术在网卡上实现了报文处理分类的任务，大大减轻了处理器的负担。
